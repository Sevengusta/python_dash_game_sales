{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e31e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_datatransfer\n",
    "from google.api_core.exceptions import Forbidden, BadRequest, NotFound\n",
    "import logging\n",
    "import os\n",
    "import re \n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a830f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GPGHC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def list_datasets(client: bigquery.Client) -> list:\n",
    "    data_summary = []  # Initialize the list to store views information\n",
    "    \n",
    "    try:\n",
    "        datasets = client.list_datasets()\n",
    "\n",
    "        if datasets:\n",
    "            for dataset in datasets:\n",
    "                dataset_id = dataset.dataset_id\n",
    "                dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "                tables = client.list_tables(dataset_ref)\n",
    "                for table in tables:\n",
    "                    table_info = {\n",
    "                        \"dataset_id\": dataset_id,\n",
    "                        \"table_id\": table.table_id,\n",
    "                        \"type\": table.table_type\n",
    "                    }\n",
    "                    data_summary.append(table_info)\n",
    "        else:\n",
    "            logging.info(f\"No datasets found in project {client.project}.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "# Example usage\n",
    "project_name = \"python-dash-game-sales-pd\"\n",
    "client = bigquery.Client(project=project_name)\n",
    "sources = list_datasets(client)\n",
    "\n",
    "# Print the sources for verification\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d673c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM sales_game_data': no such table: sales_game_data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GPGHC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2675\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[31mOperationalError\u001b[39m: no such table: sales_game_data",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m         df = pd.read_sql(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM sales_game_data\u001b[39m\u001b[33m\"\u001b[39m, _conn)\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df \n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     df = \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m      9\u001b[39m     conn.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(_conn)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_data\u001b[39m(_conn):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# ========== Tratamento inicial do df ============ #\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM sales_game_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GPGHC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    718\u001b[39m         _is_table_name = pandas_sql.has_table(sql)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GPGHC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2727\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2728\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2729\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2736\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2737\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2738\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2739\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2741\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GPGHC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2683\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minner_exc\u001b[39;00m\n\u001b[32m   2685\u001b[39m ex = DatabaseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed on sql \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2686\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mDatabaseError\u001b[39m: Execution failed on sql 'SELECT * FROM sales_game_data': no such table: sales_game_data"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"data/games_data.db\")\n",
    "try:\n",
    "    def get_data(_conn):\n",
    "        # ========== Tratamento inicial do df ============ #\n",
    "        df = pd.read_sql(f\"SELECT * FROM sales_game_data\", _conn)\n",
    "        return df \n",
    "    df = get_data(conn)\n",
    "finally:\n",
    "    conn.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1f4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=9cY77qJcK5uZy3BXJpmmm91m1GR1aw&access_type=offline&code_challenge=HQi1YxighPdWCCiMwgMMfmj2jk2l6Gg_-tFR3xDwj9g&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [sevengusta@gmail.com].\n",
      "Your current project is [python-dash-game-sales-pd].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcbdf03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:57:55,796 - WARNING - \u001b[33mNome do projeto Procurado: python-dash-game-sales-pd\u001b[0m\n",
      "2025-05-11 16:57:59,819 - ERROR - \u001b[31mErro ao tentar acessar o projeto BigQuery: list index out of range\u001b[0m\n",
      "2025-05-11 16:57:59,820 - WARNING - \u001b[33mPor favor, selecione um projeto válido.\u001b[0m\n",
      "2025-05-11 16:57:59,820 - WARNING - \u001b[33mProcessando transferências de dados de outros projetos (np)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Base directory name\n",
    "data_atual = datetime.now()\n",
    "data_formatada = data_atual.strftime(\"%Y-%m-%d\")  # Format: AAAA-MM-DD HH-MM-SS\n",
    "project_name = data_formatada\n",
    "\n",
    "# ANSI escape codes for colors\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    RESET = \"\\033[0m\"\n",
    "    GREEN = \"\\033[32m\"    # Green for INFO\n",
    "    YELLOW = \"\\033[33m\"   # Yellow for WARNING\n",
    "    RED = \"\\033[31m\"      # Red for ERROR\n",
    "\n",
    "    def format(self, record):\n",
    "        if record.levelno == logging.INFO:\n",
    "            record.msg = f\"{self.GREEN}{record.msg}{self.RESET}\"\n",
    "        elif record.levelno == logging.WARNING:\n",
    "            record.msg = f\"{self.YELLOW}{record.msg}{self.RESET}\"\n",
    "        elif record.levelno == logging.ERROR:\n",
    "            record.msg = f\"{self.RED}{record.msg}{self.RESET}\"\n",
    "        return super().format(record)\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "if not logger.hasHandlers():  # Check if handlers are already added\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(ColoredFormatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# Code to list all content inside Datasets and dataTransfers from BigQuery\n",
    "def list_datasets(client_name):\n",
    "    data_summary = []  # Initialize the list to store views information\n",
    "    datasets = client_name.list_datasets()\n",
    "\n",
    "    if datasets:\n",
    "        for dataset in datasets:\n",
    "            dataset_id = dataset.dataset_id\n",
    "            dataset_ref = client_name.dataset(dataset_id)\n",
    "\n",
    "            tables = client_name.list_tables(dataset_ref)\n",
    "            for table in tables:\n",
    "                table_info = {\n",
    "                    \"dataset_id\": dataset_id,\n",
    "                    \"table_id\": table.table_id,\n",
    "                    \"type\": table.table_type\n",
    "                }\n",
    "                data_summary.append(table_info)\n",
    "        return  data_summary\n",
    "    else:\n",
    "        logger.info(f\"Nenhum dataset encontrado no projeto {client_name.project}.\")\n",
    "        return []\n",
    "\n",
    "def list_data_transfers(project_id: str):\n",
    "    client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "    parent = f\"projects/{project_id}/locations/US\"\n",
    "    transfer_summary = []  # Initialize the list to store transfer information\n",
    "\n",
    "    for transfer_config in client.list_transfer_configs(parent=parent):\n",
    "        transfer_info = {\n",
    "                \"name_transfer\": transfer_config.display_name,\n",
    "                \"dataset_id\": transfer_config.destination_dataset_id,\n",
    "                \"table_id\": transfer_config.params.get(\"destination_table_name_template\", \"Não especificado\"),\n",
    "                \"sql\": transfer_config.params.get(\"query\", None),\n",
    "                \"type_transfer\": transfer_config.data_source_id.replace(\"_\", \" \").title() if transfer_config.destination_dataset_id != 'gateway_t' else 'CSW Data Gateway',\n",
    "                \"schedule\": transfer_config.schedule or (\"Não especificado\" if transfer_config.destination_dataset_id != 'gateway_t' else \"On Demand\"),\n",
    "                \"url\": f\"https://console.cloud.google.com/bigquery/transfers/locations/us/configs/{transfer_config.name.split('/')[-1]}/runs?inv=1&invt=Ablxwg&project={project_id}\"\n",
    "            }\n",
    "        transfer_summary.append(transfer_info)\n",
    "    return transfer_summary\n",
    "\n",
    "# Create BigQuery Files\n",
    "def save_sql(relative_path, resource_id, query):\n",
    "    file_path = os.path.join(relative_path, f\"{resource_id}.sql\")\n",
    "    \n",
    "    formatted_sql = query.replace('\\r\\n', '\\n').strip()  # Normalize newlines and strip whitespace\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as sql_file:\n",
    "        sql_file.write(formatted_sql)\n",
    "    \n",
    "    logger.info(f\"Formatted SQL saved to {file_path}\")\n",
    "\n",
    "def save_schema(dataset_id, resource_id, table):\n",
    "    schema_info = [\n",
    "        {\n",
    "            \"name\": field.name,\n",
    "            \"type\": field.field_type,\n",
    "            \"mode\": field.mode,\n",
    "            \"description\": field.description,\n",
    "        }\n",
    "        for field in table.schema\n",
    "    ]\n",
    "    # Save the schema_info to a JSON file\n",
    "    file_path = os.path.join(dataset_id, f\"{resource_id}.json\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(schema_info, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    logger.info(f\"Schema information saved to {file_path}\")\n",
    "    \n",
    "def process_external_tables(external_table_ids):   \n",
    "    logger.warning(f\"Processando transferências de dados de outros projetos (np)\")\n",
    "    while external_table_ids:\n",
    "        new_external_table_ids = set()\n",
    "        \n",
    "\n",
    "        for external_id in external_table_ids:\n",
    "            external_project_id = external_id[1]\n",
    "            base_dir_sql = os.path.join(project_name,\"SQLS\" , external_project_id)\n",
    "            base_dir_schema = os.path.join(project_name,\"SCHEMAS\" ,external_project_id)\n",
    "            try: \n",
    "                full_table_id = external_id[0]\n",
    "                source = client.get_table(full_table_id)\n",
    "                os.makedirs(base_dir_sql, exist_ok=True)\n",
    "                os.makedirs(base_dir_schema, exist_ok=True)\n",
    "                \n",
    "                dataset_dir_sql = os.path.join(base_dir_sql, external_id[2])\n",
    "                dataset_dir_schema = os.path.join(base_dir_schema, external_id[2])\n",
    "                os.makedirs(dataset_dir_sql, exist_ok=True)\n",
    "                os.makedirs(dataset_dir_schema, exist_ok=True)\n",
    "                \n",
    "                if source.table_type == 'VIEW':\n",
    "                    view_query = source.view_query\n",
    "                    \n",
    "                    for match in re.finditer(compiled_regex, view_query):\n",
    "                        new_external_table_ids.add((\n",
    "                            f\"{match.group(1)}.{match.group(2)}.{match.group(3)}\",\n",
    "                            match.group(1),  # Projeto\n",
    "                            match.group(2),  # Dataset\n",
    "                            match.group(3)   # Tabela\n",
    "                        ))\n",
    "                \n",
    "                    #SQL\n",
    "                    save_sql(dataset_dir_sql, source.table_id, view_query)\n",
    "                    \n",
    "                    #JSON\n",
    "                    save_schema(dataset_dir_schema, source.table_id, source)\n",
    "                    \n",
    "                    \n",
    "                if source.table_type == 'TABLE':\n",
    "                    table_query = f\"SELECT t.* FROM `{project_id}.{dataset_id}.{table_id}` AS t\"\n",
    "                    \n",
    "                    #SQL\n",
    "                    save_sql(dataset_dir_sql, source.table_id, table_query)\n",
    "                    \n",
    "                    #JSON\n",
    "                    save_schema(dataset_dir_schema, source.table_id, source)\n",
    "                    \n",
    "                    \n",
    "            except Forbidden as e:\n",
    "                logger.error(f'Você não possuí acesso a essa tabela: {e}')\n",
    "            except BadRequest as e:\n",
    "                logger.error(f'Você não possuí acesso a essa tabela: {e}')\n",
    "            \n",
    "        external_table_ids = new_external_table_ids   \n",
    "\n",
    "external_dict = {\n",
    "    \"python-dash-game-sales-pd\": \"python-dash-game-sales\",\n",
    "}\n",
    "\n",
    "# Main loop to get project IDs from the user\n",
    "while True:\n",
    "    external_table_ids = set()\n",
    "    projects = [\"python-dash-game-sales-pd\"]\n",
    "    \n",
    "    for project_id in projects:\n",
    "        # Initialize summaries\n",
    "        data_summary = []\n",
    "        transfer_summary = {}\n",
    "    \n",
    "        logger.warning(f'Nome do projeto Procurado: {project_id}')\n",
    "            \n",
    "        try:\n",
    "            client = bigquery.Client(project=project_id)\n",
    "            datasets = list(client.list_datasets())  # Convert the iterator to a list\n",
    "            datasets[0] # Verify if exist any dataset in this project\n",
    "            \n",
    "            data_summary = list_datasets(client)\n",
    "            transfer_summary = list_data_transfers(project_id)\n",
    "            table_names = [transfer['table_id'] for transfer in transfer_summary] # Store the of each table with transfer\n",
    "            logger.info('Dados Foram Acessados com sucesso.')\n",
    "            base_dir_sql = os.path.join(project_name,\"SQLS\" , project_id)\n",
    "            base_dir_schema = os.path.join(project_name,\"SCHEMAS\" ,project_id)\n",
    "            os.makedirs(base_dir_sql, exist_ok=True)\n",
    "            os.makedirs(base_dir_schema, exist_ok=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f'Erro ao tentar acessar o projeto BigQuery: {e}')\n",
    "            logger.warning('Por favor, selecione um projeto válido.')\n",
    "        \n",
    "        \n",
    "        # Regular expression to match external table references\n",
    "        regex = rf\"({external_dict[project_id]})\\.([^`]+)\\.([^`]+)\"\n",
    "        compiled_regex = re.compile(regex)\n",
    "\n",
    "        # Save views and their schemas\n",
    "        for table in data_summary:\n",
    "            dataset_id = table['dataset_id']\n",
    "            table_id = table['table_id']\n",
    "            full_table_id = f\"{project_id}.{table['dataset_id']}.{table['table_id']}\"\n",
    "            dataset_dir_sql = os.path.join(base_dir_sql, dataset_id)\n",
    "            dataset_dir_schema = os.path.join(base_dir_schema, dataset_id)\n",
    "            os.makedirs(dataset_dir_sql, exist_ok=True)\n",
    "            os.makedirs(dataset_dir_schema, exist_ok=True)\n",
    "            source = client.get_table(full_table_id)\n",
    "            if table['type'] == \"VIEW\":\n",
    "                view_query = source.view_query \n",
    "                \n",
    "                for match in re.finditer(compiled_regex, view_query):\n",
    "                    # Check if any part of the first group matches \"bcs-latam-it-lake\"\n",
    "                    if external_dict[project_id] in match.group(1):\n",
    "                        external_table_ids.add((\n",
    "                            f\"{match.group(1)}.{match.group(2)}.{match.group(3)}\",\n",
    "                            match.group(1),  # Project\n",
    "                            match.group(2),  # Dataset\n",
    "                            match.group(3)   # Table\n",
    "                        ))\n",
    "                # SQL\n",
    "                save_sql(dataset_dir_sql, table_id, view_query)\n",
    "                \n",
    "                # JSON\n",
    "                save_schema(dataset_dir_schema, table_id, source)\n",
    "                \n",
    "            elif table['type'] == \"TABLE\" and table_id not in table_names :\n",
    "                table_query = f\"SELECT t.* FROM `{project_id}.{dataset_id}.{table_id}` AS t\"\n",
    "                # SQL\n",
    "                save_sql(dataset_dir_sql, table_id, table_query)\n",
    "                \n",
    "                # JSON\n",
    "                save_schema(dataset_dir_schema, table_id, source)\n",
    "                \n",
    "\n",
    "        # Save transfer information\n",
    "        \n",
    "        for table in transfer_summary:\n",
    "            \n",
    "            dataset_id = table['dataset_id']\n",
    "            table_id = table['table_id']\n",
    "            full_table_id = f\"{project_id}.{table['dataset_id']}.{table['table_id']}\"\n",
    "            dataset_dir_sql = os.path.join(base_dir_sql, dataset_id)\n",
    "            dataset_dir_schema = os.path.join(base_dir_schema, dataset_id)\n",
    "            os.makedirs(dataset_dir_sql, exist_ok=True)\n",
    "            os.makedirs(dataset_dir_schema, exist_ok=True)\n",
    "            source = client.get_table(full_table_id)\n",
    "            if table['sql']:\n",
    "                \n",
    "                # SQL\n",
    "                for match in re.finditer(compiled_regex, table['sql']):\n",
    "                    # Check if any part of the first group matches \"bcs-latam-it-lake\"\n",
    "                    if  match.group(1) in external_dict[project_id]:\n",
    "                        external_table_ids.add((\n",
    "                            f\"{match.group(1)}.{match.group(2)}.{match.group(3)}\",\n",
    "                            match.group(1),  # Project\n",
    "                            match.group(2),  # Dataset\n",
    "                            match.group(3)   # Table\n",
    "                        ))\n",
    "                        \n",
    "                        \n",
    "                save_sql(dataset_dir_sql, table_id, table['sql'])\n",
    "                    \n",
    "                # JSON\n",
    "                save_schema(dataset_dir_schema, table_id, source)\n",
    "            else:\n",
    "                table_query = f\"SELECT t.* FROM `{project_id}.{dataset_id}.{table_id}` AS t\"\n",
    "                save_sql(dataset_dir_sql, table_id, table_query)\n",
    "                    \n",
    "                # JSON\n",
    "                save_schema(dataset_dir_schema, table_id, source)\n",
    "                \n",
    "        # processing external tables\n",
    "        process_external_tables(external_table_ids)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
